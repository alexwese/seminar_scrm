{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21647,"status":"ok","timestamp":1665826078553,"user":{"displayName":"Alex Wese","userId":"10902371568231088354"},"user_tz":-120},"id":"sje0EAEqQlds","outputId":"294de3f6-db8a-4b48-fef2-b5ec00a94adb"},"outputs":[],"source":["# Importing necessary library\n","import pandas as pd\n","import numpy as np\n","import nltk\n","import os\n","from nltk.tokenize import word_tokenize# Passing the string text into word tokenize for breaking the sentences\n","import nltk.corpus# sample text for performing tokenization\n","from nltk.probability import FreqDist\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import LancasterStemmer\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","from os import listdir\n","from os.path import isfile, join\n","from wordcloud import WordCloud, STOPWORDS\n","import matplotlib.pyplot as plt\n","from nltk.probability import FreqDist\n","from nltk.stem.snowball import SnowballStemmer\n","import string\n","from nltk.stem.wordnet import WordNetLemmatizer\n","import inflect\n","import matplotlib.pyplot as plt\n","import gensim\n","import gensim.corpora as corpora\n","from gensim.utils import simple_preprocess\n","from gensim.models import CoherenceModel# spaCy for preprocessing\n","import pyLDAvis\n","import pyLDAvis.gensim\n","import matplotlib.pyplot as plt\n","from tika import parser\n","\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('words')\n","nltk.download('stopwords')\n","nltk.download('omw-1.4')\n","\n","words = set(nltk.corpus.words.words())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55422,"status":"ok","timestamp":1665826133968,"user":{"displayName":"Alex Wese","userId":"10902371568231088354"},"user_tz":-120},"id":"hQvl2vvoULDr","outputId":"30c4f913-a87e-44a9-e6c9-b48eb5064c9a"},"outputs":[],"source":["dir = \"Literature\"\n","\n","## IMPORT TEXT ##\n","files = [f for f in listdir(dir) if isfile(join(dir, f))]\n","print(files)\n","text = \"\"\n","doc_list = []\n","\n","for file in files:\n","    if file.endswith('.pdf'):\n","        raw = parser.from_file(\"./\"+dir+\"/\"+file)\n","        res = raw['content'].replace(\"\\n\", \" \" )\n","        res = res.replace(\"\\t\", \" \" )\n","        text = text + res\n","        doc_list.append(res)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":16917,"status":"ok","timestamp":1665826150880,"user":{"displayName":"Alex Wese","userId":"10902371568231088354"},"user_tz":-120},"id":"qMwYGxRpQya3"},"outputs":[],"source":["## STOPWORDS ##\n","a = set(stopwords.words(\"english\"))\n","additional_stopwords  = [',','.', '(', ')','&', ':', 'n', '1','2','c','ti','en', 'â€¢','et',\n","                        'r','g','er','e','b','p','l','u','s','l','e','i','r','t','a','s','h',\n","                        'o','s','_','.','https','al','fig','','also','x','may','used','one','two',\n","                        'might','research','article','according','within','among','literature','thu',\n","                        'number','sc','need','part','table','many','another','study','three','see', \n","                        'journal','focu','example','datum','time','show','following','several','source',\n","                        'lo','specific','focus','first','paper','based','though','dates','become','using',\n","                        'often','set','refer','able','must','needs','need','shown','thus','possible','make',\n","                        'even','various','tion','consider','related','certain','figure','figures',\n","                        'use','including','identified','way','required','ing','due','defined','chapter',\n","                        'theory', 'int ', 'int', 'int j', 'int i', 'show', 'shows', 'ment','j','given',\n","                        'author','term','http', 'eg','pp', 'ie','kleindorfer','saad','vol' ,'f f', 'f', 'fi',\n","                        'rm','mentzer','manuj','well','zsidisin','said','ic','el','te','review','st','sy',\n","                         'st','since','de','initial','wang','k',\n","                        'em','finding','k','le','supply','chain','risk','management','system','annals','important']\n","\n","#TOKENIZE ##\n","list_lower = []\n","for i in doc_list:\n","  list_lower.append(word_tokenize(i.lower())) \n","\n","\n","\n","\n","#STOPWORDS##\n","list_noSW = []\n","for doc in list_lower:\n","  list_noSW.append([str(x) for x in doc if str(x) not in a]) \n","\n","\n","\n","\n","## PUNCTUATION ##\n","list_words = []\n","for doc in list_noSW:\n","  list_words.append([word for word in doc if word.isalpha()]) \n","\n","\n","\n","\n","# LEMMATIZER ##\n","lemmatizer = WordNetLemmatizer() \n","\n","list_lemmatized = []\n","for doc in list_words:\n","  list_lemmatized.append([lemmatizer.lemmatize(word) for word in doc]) \n","\n","\n","\n","\n","## ADDITIONAL STOPWORDS ##\n","engine = inflect.engine()\n","stemmed_new = []\n","\n","\n","list_stemmed_new = []\n","for doc in list_lemmatized:\n","  \n","  stemmed_doc = []\n","\n","  for i in doc:\n","      if i not in additional_stopwords:\n","            stemmed_doc.append(i)\n","\n","  list_stemmed_new.append(stemmed_doc) \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3292,"status":"ok","timestamp":1665826154166,"user":{"displayName":"Alex Wese","userId":"10902371568231088354"},"user_tz":-120},"id":"gCxNpw-0Q1rk","outputId":"aac4768e-2b13-46c0-f5e7-f4525261abb4"},"outputs":[],"source":["# Create Dictionary\n","id2word = corpora.Dictionary(list_stemmed_new)\n","\n","\n","# Create Corpus\n","texts = list_stemmed_new\n","\n","# Term Document Frequency\n","corpus = [id2word.doc2bow(text) for text in texts]\n","\n","lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n","                                      id2word=id2word,\n","                                      num_topics=10\n","                                      )\n","\n","\n","\n","# Compute Perplexity\n","print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":919},"executionInfo":{"elapsed":4503,"status":"ok","timestamp":1665826158666,"user":{"displayName":"Alex Wese","userId":"10902371568231088354"},"user_tz":-120},"id":"dDdJsGNuLC6W","outputId":"9e893fd9-1b7c-40e7-fd52-cbd6ad71b6e5"},"outputs":[],"source":["# Visualize the topics\n","pyLDAvis.enable_notebook()\n","vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1665826158666,"user":{"displayName":"Alex Wese","userId":"10902371568231088354"},"user_tz":-120},"id":"HjOe6zKytDT1"},"outputs":[],"source":["pyLDAvis.save_html(vis, 'lda_new.html')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPbEl/U0ej2fZ3nnbaZTaSR","collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.8.5 ('base')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.5"},"vscode":{"interpreter":{"hash":"40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"}}},"nbformat":4,"nbformat_minor":0}
